#!/usr/bin/env python3
"""
JSON ì¤‘ë³µ í‚¤ ì œê±° ë„êµ¬ (ê°œì„  ë²„ì „)
- ì¤‘ë³µ í‚¤ íƒì§€ ë° ì œê±°
- ë§ˆì§€ë§‰ ê°’ ìœ ì§€
- ë°±ì—… ìë™ ìƒì„±
"""

import json
import sys
import shutil
from pathlib import Path
from datetime import datetime
from collections import OrderedDict

def remove_duplicates(json_path):
    """JSON íŒŒì¼ì—ì„œ ì¤‘ë³µ í‚¤ ì œê±°"""
    json_path = Path(json_path)
    
    if not json_path.exists():
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {json_path}")
        return False
    
    # ë°±ì—…
    backup_path = json_path.with_suffix('.json.bak')
    shutil.copy(json_path, backup_path)
    print(f"ğŸ’¾ ë°±ì—…: {backup_path}")
    
    # ë¡œë“œ
    with open(json_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ì¤‘ë³µ í‚¤ ì°¾ê¸°
    duplicates_found = {}
    
    try:
        # JSON íŒŒì‹± (ì¤‘ë³µ í‚¤ëŠ” ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ìë™ ë®ì–´ì”€)
        data = json.loads(content, object_pairs_hook=OrderedDict)
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¤‘ë³µ í™•ì¸
        for category, entries in data.items():
            if isinstance(entries, dict):
                # ì›ë³¸ contentì—ì„œ ì´ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ë“¤ì„ ì°¾ì•„ ì¤‘ë³µ í™•ì¸
                import re
                pattern = f'"{category}"\\s*:\\s*{{([^}}]*)}}'
                match = re.search(pattern, content, re.DOTALL)
                if match:
                    category_content = match.group(1)
                    keys = re.findall(r'"([^"]+)"\\s*:', category_content)
                    
                    # ì¤‘ë³µ ì°¾ê¸°
                    seen = {}
                    for key in keys:
                        if key in seen:
                            if category not in duplicates_found:
                                duplicates_found[category] = []
                            if key not in duplicates_found[category]:
                                duplicates_found[category].append(key)
                        seen[key] = True
        
        if duplicates_found:
            print(f"\nâš ï¸  ì¤‘ë³µ í‚¤ ë°œê²¬:")
            for cat, keys in duplicates_found.items():
                print(f"  [{cat}]: {len(keys)}ê°œ")
                for key in keys[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                    print(f"    - {key}")
                if len(keys) > 3:
                    print(f"    ... ì™¸ {len(keys) - 3}ê°œ")
        
        # ì €ì¥ (ì¤‘ë³µì€ ìë™ìœ¼ë¡œ ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ë®ì–´ì”€)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì •ë¦¬ ì™„ë£Œ: {json_path.name}")
        return True
        
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        # ë°±ì—… ë³µì›
        shutil.copy(backup_path, json_path)
        return False

def clean_all_glossaries():
    """ëª¨ë“  glossary íŒŒì¼ ì •ë¦¬"""
    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ LOCALIZATION í´ë” ì°¾ê¸°
    loc_dir = Path(__file__).parent.parent / "LOCALIZATION"
    
    print("=" * 80)
    print("ğŸ§¹ JSON ì¤‘ë³µ í‚¤ ì œê±°")
    print("=" * 80 + "\n")
    
    success_count = 0
    fail_count = 0
    
    for json_file in sorted(loc_dir.glob("glossary_*.json")):
        print(f"\nğŸ“„ {json_file.name}")
        if remove_duplicates(json_file):
            success_count += 1
        else:
            fail_count += 1
    
    print("\n" + "=" * 80)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    if fail_count > 0:
        print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    print("=" * 80)

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # íŠ¹ì • íŒŒì¼ë§Œ ì²˜ë¦¬
        remove_duplicates(sys.argv[1])
    else:
        # ëª¨ë“  glossary íŒŒì¼ ì²˜ë¦¬
        clean_all_glossaries()
