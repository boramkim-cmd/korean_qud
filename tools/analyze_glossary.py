#!/usr/bin/env python3
"""
ğŸ“Š Glossary êµì°¨ ê²€ì¦ ë° ë¶„ì„ ë„êµ¬ (Glossary Cross-Validation Tool)
- ì¤‘ë³µ í‚¤ íƒì§€ (íŒŒì¼ ê°„ / íŒŒì¼ ë‚´)
- ëŒ€ì†Œë¬¸ì ì¤‘ë³µ íƒì§€
- CS ì½”ë“œ ì˜ì¡´ì„± ë¶„ì„
- ì¹´í…Œê³ ë¦¬ êµ¬ì¡° ë¶„ì„
"""

from __future__ import annotations
import json
import re
from collections import defaultdict
from pathlib import Path
from typing import Any

# ============================================================================
# ì„¤ì •
# ============================================================================

PROJECT_ROOT = Path(__file__).parent.parent.resolve()
LOCALIZATION_DIR = PROJECT_ROOT / "LOCALIZATION"
SCRIPTS_DIR = PROJECT_ROOT / "Scripts"
REPORT_FILE = LOCALIZATION_DIR / "integrity_report.md"

# CSì—ì„œ ì°¸ì¡°í•˜ëŠ” ì¹´í…Œê³ ë¦¬ (ì½”ë“œ ë¶„ì„ ê²°ê³¼)
CS_REFERENCED_CATEGORIES = {
    "common", "ui", "status", "inventory", "options", "display",
    "chargen_ui", "attribute", "skill", "powers", "skill_desc"
}


def load_all_glossaries() -> dict[str, dict[str, Any]]:
    """ëª¨ë“  glossary JSON íŒŒì¼ ë¡œë“œ"""
    glossaries = {}
    for json_file in sorted(LOCALIZATION_DIR.glob("glossary_*.json")):
        try:
            data = json.loads(json_file.read_text(encoding='utf-8'))
            glossaries[json_file.name] = data
        except json.JSONDecodeError as e:
            print(f"âš ï¸  JSON ì˜¤ë¥˜ ({json_file.name}): {e}")
    return glossaries


def analyze_duplicates(glossaries: dict[str, dict[str, Any]]) -> dict[str, list]:
    """ì¤‘ë³µ í‚¤ ë¶„ì„"""
    # 1. ì „ì—­ í‚¤-ìœ„ì¹˜ ë§µí•‘
    global_keys: dict[str, list[str]] = defaultdict(list)  # key -> [(file, category), ...]
    
    # 2. ëŒ€ì†Œë¬¸ì ì¤‘ë³µ ì¶”ì 
    case_insensitive: dict[str, list[tuple[str, str, str]]] = defaultdict(list)  # lower_key -> [(file, cat, original_key), ...]
    
    for filename, data in glossaries.items():
        for category, entries in data.items():
            if not isinstance(entries, dict):
                continue
            for key in entries.keys():
                location = f"{filename}:{category}"
                global_keys[key].append(location)
                case_insensitive[key.lower()].append((filename, category, key))
    
    # ì •í™•í•œ ì¤‘ë³µ ì°¾ê¸°
    exact_duplicates = {k: v for k, v in global_keys.items() if len(v) > 1}
    
    # ëŒ€ì†Œë¬¸ìë§Œ ë‹¤ë¥¸ ì¤‘ë³µ ì°¾ê¸° (ì •í™•í•œ ì¤‘ë³µ ì œì™¸)
    case_duplicates = {}
    for lower_key, locations in case_insensitive.items():
        if len(locations) > 1:
            unique_originals = set(loc[2] for loc in locations)
            if len(unique_originals) > 1:  # ëŒ€ì†Œë¬¸ìê°€ ì‹¤ì œë¡œ ë‹¤ë¥¸ ê²½ìš°
                case_duplicates[lower_key] = locations
    
    return {
        "exact": exact_duplicates,
        "case": case_duplicates
    }


def analyze_structure(glossaries: dict[str, dict[str, Any]]) -> dict[str, Any]:
    """êµ¬ì¡° ë¶„ì„"""
    structure = {}
    for filename, data in glossaries.items():
        file_info = {"categories": {}, "total_entries": 0}
        for category, entries in data.items():
            if category.startswith("_"):  # ë©”íƒ€ë°ì´í„° ìŠ¤í‚µ
                continue
            if isinstance(entries, dict):
                count = len(entries)
                file_info["categories"][category] = count
                file_info["total_entries"] += count
        structure[filename] = file_info
    return structure


def analyze_cs_dependencies() -> dict[str, list[str]]:
    """CS íŒŒì¼ì—ì„œ ì°¸ì¡°í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
    dependencies: dict[str, list[str]] = defaultdict(list)
    pattern = re.compile(r'LocalizationManager\.GetCategory\("([^"]+)"\)')
    
    for cs_file in SCRIPTS_DIR.rglob("*.cs"):
        try:
            content = cs_file.read_text(encoding='utf-8')
            for match in pattern.finditer(content):
                category = match.group(1)
                rel_path = str(cs_file.relative_to(PROJECT_ROOT))
                if rel_path not in dependencies[category]:
                    dependencies[category].append(rel_path)
        except IOError:
            continue
    
    return dict(dependencies)


def find_missing_categories(glossaries: dict, cs_deps: dict) -> tuple[set, set]:
    """CSì—ì„œ ì°¸ì¡°í•˜ì§€ë§Œ glossaryì— ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ì°¾ê¸°"""
    existing_categories = set()
    for data in glossaries.values():
        existing_categories.update(data.keys())
    
    referenced = set(cs_deps.keys())
    missing = referenced - existing_categories
    unused = existing_categories - referenced - {"_meta"}
    
    return missing, unused


def generate_report(
    glossaries: dict,
    duplicates: dict,
    structure: dict,
    cs_deps: dict,
    missing: set,
    unused: set
) -> str:
    """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
    lines = [
        "# ğŸ“Š Glossary ë¬´ê²°ì„± ë¦¬í¬íŠ¸",
        "",
        f"**ìƒì„± ì‹œê°**: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "---",
        "",
        "## ğŸ“ íŒŒì¼ë³„ êµ¬ì¡°",
        "",
        "| íŒŒì¼ | ì¹´í…Œê³ ë¦¬ ìˆ˜ | í•­ëª© ìˆ˜ |",
        "|------|----------:|------:|"
    ]
    
    total_entries = 0
    for filename, info in sorted(structure.items()):
        cat_count = len(info["categories"])
        entry_count = info["total_entries"]
        total_entries += entry_count
        lines.append(f"| `{filename}` | {cat_count} | {entry_count} |")
    
    lines.append(f"| **ì´ê³„** | | **{total_entries}** |")
    lines.append("")
    
    # ì¹´í…Œê³ ë¦¬ ìƒì„¸
    lines.extend([
        "### ì¹´í…Œê³ ë¦¬ ìƒì„¸",
        ""
    ])
    
    for filename, info in sorted(structure.items()):
        if info["categories"]:
            lines.append(f"**{filename}**:")
            for cat, count in sorted(info["categories"].items()):
                lines.append(f"  - `{cat}`: {count}ê°œ")
            lines.append("")
    
    # ì¤‘ë³µ ë¶„ì„
    lines.extend([
        "---",
        "",
        "## âš ï¸ ì¤‘ë³µ í‚¤ ë¶„ì„",
        ""
    ])
    
    if duplicates["exact"]:
        lines.append(f"### ì •í™•í•œ ì¤‘ë³µ: {len(duplicates['exact'])}ê°œ")
        lines.append("")
        for key, locations in sorted(duplicates["exact"].items())[:20]:
            lines.append(f"- `{key[:50]}{'...' if len(key) > 50 else ''}`")
            for loc in locations:
                lines.append(f"  - {loc}")
        if len(duplicates["exact"]) > 20:
            lines.append(f"- ... ì™¸ {len(duplicates['exact']) - 20}ê°œ")
        lines.append("")
    else:
        lines.append("âœ… ì •í™•í•œ ì¤‘ë³µ í‚¤ ì—†ìŒ")
        lines.append("")
    
    if duplicates["case"]:
        lines.append(f"### ëŒ€ì†Œë¬¸ìë§Œ ë‹¤ë¥¸ ì¤‘ë³µ: {len(duplicates['case'])}ê°œ")
        lines.append("")
        lines.append("> ì´ í•­ëª©ë“¤ì€ Options í™”ë©´ ë“±ì—ì„œ ëŒ€ì†Œë¬¸ì ëª¨ë‘ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        lines.append("")
        for lower_key, locations in sorted(duplicates["case"].items())[:15]:
            variants = list(set(loc[2] for loc in locations))
            lines.append(f"- `{lower_key[:40]}`: {len(variants)}ê°œ ë³€í˜•")
            for v in variants[:3]:
                lines.append(f"  - `{v}`")
        if len(duplicates["case"]) > 15:
            lines.append(f"- ... ì™¸ {len(duplicates['case']) - 15}ê°œ")
        lines.append("")
    
    # CS ì˜ì¡´ì„±
    lines.extend([
        "---",
        "",
        "## ğŸ”— CS ì½”ë“œ ì˜ì¡´ì„±",
        "",
        "ì½”ë“œì—ì„œ `LocalizationManager.GetCategory()`ë¡œ ì°¸ì¡°í•˜ëŠ” ì¹´í…Œê³ ë¦¬:",
        ""
    ])
    
    for category, files in sorted(cs_deps.items()):
        lines.append(f"### `{category}`")
        for f in files[:5]:
            lines.append(f"- {f}")
        if len(files) > 5:
            lines.append(f"- ... ì™¸ {len(files) - 5}ê°œ íŒŒì¼")
        lines.append("")
    
    # ë¬¸ì œì 
    lines.extend([
        "---",
        "",
        "## ğŸ”´ ë°œê²¬ëœ ë¬¸ì œì ",
        ""
    ])
    
    if missing:
        lines.append("### ëˆ„ë½ëœ ì¹´í…Œê³ ë¦¬ (CSì—ì„œ ì°¸ì¡°í•˜ì§€ë§Œ glossaryì— ì—†ìŒ)")
        for cat in sorted(missing):
            lines.append(f"- âŒ `{cat}`")
        lines.append("")
    
    if unused:
        lines.append("### ë¯¸ì‚¬ìš© ì¹´í…Œê³ ë¦¬ (glossaryì— ìˆì§€ë§Œ CSì—ì„œ ì°¸ì¡° ì•ˆ í•¨)")
        for cat in sorted(unused):
            lines.append(f"- âšª `{cat}`")
        lines.append("")
    
    # ê¶Œì¥ ì‚¬í•­
    lines.extend([
        "---",
        "",
        "## ğŸ’¡ ê¶Œì¥ êµ¬ì¡° ê°œí¸",
        "",
        "í˜„ì¬ íŒŒì¼ë“¤ì„ í™”ë©´ ê¸°ì¤€ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:",
        "",
        "```",
        "LOCALIZATION/",
        "â”œâ”€â”€ screens/              # í™”ë©´ë³„ ë²ˆì—­",
        "â”‚   â”œâ”€â”€ mainmenu.json     # ë©”ì¸ ë©”ë‰´",
        "â”‚   â”œâ”€â”€ options.json      # ì„¤ì • í™”ë©´",
        "â”‚   â”œâ”€â”€ chargen.json      # ìºë¦­í„° ìƒì„±",
        "â”‚   â”œâ”€â”€ gameplay.json     # ê²Œì„í”Œë ˆì´ UI",
        "â”‚   â””â”€â”€ inventory.json    # ì¸ë²¤í† ë¦¬/ì¥ë¹„",
        "â”œâ”€â”€ data/                 # ê²Œì„ ë°ì´í„°",
        "â”‚   â”œâ”€â”€ skills.json       # ìŠ¤í‚¬",
        "â”‚   â”œâ”€â”€ mutations.json    # ë³€ì´",
        "â”‚   â”œâ”€â”€ cybernetics.json  # ì‚¬ì´ë²„ë„¤í‹±ìŠ¤",
        "â”‚   â””â”€â”€ factions.json     # ì„¸ë ¥",
        "â”œâ”€â”€ shared/               # ê³µìš© ìš©ì–´",
        "â”‚   â”œâ”€â”€ common.json       # ê³µìš© UI",
        "â”‚   â””â”€â”€ terms.json        # ê²Œì„ ìš©ì–´",
        "â””â”€â”€ SUBTYPES/             # ê¸°ì¡´ í•˜ìœ„ìœ í˜• (ìœ ì§€)",
        "    â””â”€â”€ ...               ",
        "```",
        ""
    ])
    
    return "\n".join(lines)


def main() -> None:
    print("=" * 60)
    print("ğŸ“Š Glossary êµì°¨ ê²€ì¦ ì‹œì‘")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n1ï¸âƒ£ Glossary íŒŒì¼ ë¡œë“œ ì¤‘...")
    glossaries = load_all_glossaries()
    print(f"   â†’ {len(glossaries)}ê°œ íŒŒì¼ ë¡œë“œë¨")
    
    # 2. êµ¬ì¡° ë¶„ì„
    print("\n2ï¸âƒ£ êµ¬ì¡° ë¶„ì„ ì¤‘...")
    structure = analyze_structure(glossaries)
    total = sum(info["total_entries"] for info in structure.values())
    print(f"   â†’ ì´ {total}ê°œ ë²ˆì—­ í•­ëª©")
    
    # 3. ì¤‘ë³µ ë¶„ì„
    print("\n3ï¸âƒ£ ì¤‘ë³µ í‚¤ ë¶„ì„ ì¤‘...")
    duplicates = analyze_duplicates(glossaries)
    print(f"   â†’ ì •í™•í•œ ì¤‘ë³µ: {len(duplicates['exact'])}ê°œ")
    print(f"   â†’ ëŒ€ì†Œë¬¸ì ì¤‘ë³µ: {len(duplicates['case'])}ê°œ")
    
    # 4. CS ì˜ì¡´ì„± ë¶„ì„
    print("\n4ï¸âƒ£ CS ì½”ë“œ ì˜ì¡´ì„± ë¶„ì„ ì¤‘...")
    cs_deps = analyze_cs_dependencies()
    print(f"   â†’ {len(cs_deps)}ê°œ ì¹´í…Œê³ ë¦¬ê°€ ì½”ë“œì—ì„œ ì°¸ì¡°ë¨")
    
    # 5. ëˆ„ë½/ë¯¸ì‚¬ìš© ì¹´í…Œê³ ë¦¬
    print("\n5ï¸âƒ£ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ê²€ì¦ ì¤‘...")
    missing, unused = find_missing_categories(glossaries, cs_deps)
    if missing:
        print(f"   âš ï¸  ëˆ„ë½ ì¹´í…Œê³ ë¦¬: {', '.join(missing)}")
    if unused:
        print(f"   â„¹ï¸  ë¯¸ì‚¬ìš© ì¹´í…Œê³ ë¦¬: {len(unused)}ê°œ")
    
    # 6. ë¦¬í¬íŠ¸ ìƒì„±
    print("\n6ï¸âƒ£ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = generate_report(glossaries, duplicates, structure, cs_deps, missing, unused)
    REPORT_FILE.write_text(report, encoding='utf-8')
    print(f"   â†’ ì €ì¥ë¨: {REPORT_FILE.relative_to(PROJECT_ROOT)}")
    
    print("\n" + "=" * 60)
    print("âœ… êµì°¨ ê²€ì¦ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
